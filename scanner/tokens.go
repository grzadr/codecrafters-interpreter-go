package scanner

import (
	"fmt"
	"iter"
	"os"
	"slices"
	"strconv"
	"unicode"
)

//go:generate stringer -type=tokenType
type tokenType int

const (
	EOF tokenType = iota
	LEFT_PAREN
	RIGHT_PAREN
	LEFT_BRACE
	RIGHT_BRACE
	COMMA
	DOT
	MINUS
	PLUS
	SEMICOLON
	STAR
	EQUAL
	EQUAL_EQUAL
	BANG
	BANG_EQUAL
	LESS
	LESS_EQUAL
	GREATER
	GREATER_EQUAL
	SLASH
	COMMENT
	STRING
	AND
	CLASS
	ELSE
	FALSE
	FOR
	FUN
	IF
	NIL
	OR
	PRINT
	RETURN
	SUPER
	THIS
	TRUE
	VAR
	WHILE
	NUMBER
	IDENTIFIER
)

type (
	lexemePrefix byte
	lexeme       string
)

var defaultLexemes = [...]lexeme{
	"",
	"(",
	")",
	"{",
	"}",
	",",
	".",
	"-",
	"+",
	";",
	"*",
	"=",
	"==",
	"!",
	"!=",
	"<",
	"<=",
	">",
	">=",
	"/",
	"//",
	"\"",
	"and",
	"class",
	"else",
	"false",
	"for",
	"fun",
	"if",
	"nil",
	"or",
	"print",
	"return",
	"super",
	"this",
	"true",
	"var",
	"while",
}

type Literal interface {
	String() string
	isLiteral()
}

type StringLiteral string

func (l StringLiteral) String() string {
	return string(l)
}

func (l StringLiteral) isLiteral() {}

type NumberLiteral float64

func newNumberLiteral(num string) NumberLiteral {
	literal, _ := strconv.ParseFloat(num, 64)

	return NumberLiteral(literal)
}

func (l NumberLiteral) String() string {
	if l == NumberLiteral(int64(l)) {
		return fmt.Sprintf("%g.0", l)
	} else {
		return fmt.Sprintf("%g", l)
	}
}

func (l NumberLiteral) isLiteral() {}

type BoolLiteral bool

func newBoolLiteral(ttype tokenType) BoolLiteral {
	if ttype == TRUE {
		return BoolLiteral(true)
	}

	return BoolLiteral(false)
}

func (l BoolLiteral) String() string {
	return fmt.Sprintf("%t", l)
}

func (l BoolLiteral) isLiteral() {}

type Token struct {
	tokenType tokenType
	lexeme    lexeme
	literal   Literal
	err       error
}

func newToken(ttype tokenType) Token {
	return Token{
		tokenType: ttype,
		lexeme:    defaultLexemes[int(ttype)],
		literal:   StringLiteral("null"),
	}
}

func newEOFToken() Token {
	return newToken(EOF)
}

func newUnexpectedCharToken(line int, b byte) Token {
	return Token{
		err: fmt.Errorf(
			"[line %d] Error: Unexpected character: %s",
			line,
			string(b),
		),
	}
}

func newUnterminatedStringToken(line int) Token {
	return Token{
		err: fmt.Errorf(
			"[line %d] Error: Unterminated string.",
			line,
		),
	}
}

func newNumberToken(num string) Token {
	return Token{
		tokenType: NUMBER,
		lexeme:    lexeme(num),
		literal:   newNumberLiteral(num),
	}
}

func (t Token) IsError() bool {
	return t.err != nil
}

func (t Token) String() string {
	if t.IsError() {
		return t.error()
	}

	return fmt.Sprintf("%s %s %s", t.tokenType, t.lexeme, t.literal)
}

func (t Token) error() string {
	return t.err.Error()
}

func scanIfNext(ttype tokenType, expected byte) scanFunc {
	return func(t *Tokenizer) Token {
		if next, ok := t.peek(); !ok || next != expected {
			return newToken(ttype)
		} else {
			t.skip()

			return newToken(ttype + 1)
		}
	}
}

func scanIfNextIsEqual(ttype tokenType) scanFunc {
	return scanIfNext(ttype, '=')
}

func scanDefault(ttype tokenType) scanFunc {
	return func(t *Tokenizer) Token {
		return newToken(ttype)
	}
}

func scanString() scanFunc {
	return func(t *Tokenizer) Token {
		content, found := t.restTo('"')
		if !found {
			t.skipLine()

			return newUnterminatedStringToken(t.lineNum)
		}

		return Token{
			tokenType: STRING,
			lexeme:    lexeme(fmt.Sprintf("\"%s\"", string(content))),
			literal:   StringLiteral(content),
		}
	}
}

func scanNumber(b byte) scanFunc {
	return func(t *Tokenizer) Token {
		data := []byte{b}

		for {
			next, ok := t.peek()
			if !ok || (!unicode.IsDigit(rune(next)) && next != '.') {
				break
			}

			t.skip()

			data = append(data, next)
		}

		return newNumberToken(string(data))
	}
}

func scanIdentifier(b byte) scanFunc {
	return func(t *Tokenizer) Token {
		data := []byte{b}

		for {
			next, ok := t.peek()

			if !ok ||
				!(unicode.IsLetter(rune(next)) || unicode.IsDigit(rune(next)) || next == '_') {
				break
			}

			t.skip()

			data = append(data, next)
		}

		lexeme := lexeme(data)

		ttype, found := reservedLexemeIndex[lexeme]

		if found {
			return newToken(ttype)
		}

		return Token{
			tokenType: IDENTIFIER,
			lexeme:    lexeme,
			literal:   StringLiteral("null"),
		}
	}
}

type (
	scanFunc      func(t *Tokenizer) Token
	lexemeIndex   map[lexemePrefix]scanFunc
	reservedIndex map[lexeme]tokenType
)

func newLexemeIndex() lexemeIndex {
	index := lexemeIndex{
		'=': scanIfNextIsEqual(EQUAL),
		'!': scanIfNextIsEqual(BANG),
		'<': scanIfNextIsEqual(LESS),
		'>': scanIfNextIsEqual(GREATER),
		'/': scanIfNext(SLASH, '/'),
		'"': scanString(),
	}

	for i, lexeme := range defaultLexemes[1:] {
		prefix := lexemePrefix(lexeme[0])
		if _, found := index[prefix]; found {
			continue
		}

		ttype := tokenType(i + 1)

		if ttype == AND {
			break
		}

		index[prefix] = scanDefault(ttype)
	}

	return index
}

func (i lexemeIndex) find(l lexemePrefix) (f scanFunc, found bool) {
	if unicode.IsDigit(rune(l)) {
		return scanNumber(byte(l)), true
	}

	f, found = i[l]

	if !found && (unicode.IsLetter(rune(l)) || l == '_') {
		return scanIdentifier(byte(l)), true
	}

	return
}

var mainLexemeIndex = newLexemeIndex()

func newReservedLexemeIndex() reservedIndex {
	index := make(reservedIndex, len(defaultLexemes)-int(AND))

	for i, lexeme := range defaultLexemes[AND:] {
		index[lexeme] = tokenType(i + int(AND))
	}

	return index
}

var reservedLexemeIndex = newReservedLexemeIndex()

func readFileContent(filename string) (content []byte, err error) {
	content, err = os.ReadFile(filename)
	if err != nil {
		err = fmt.Errorf(
			"error reading file %q: %v\n",
			filename,
			err,
		)
	}

	return
}

type Tokenizer struct {
	data    []byte
	offset  int
	lineNum int
}

func newTokenizer(filename string) (t Tokenizer, err error) {
	t.data, err = readFileContent(filename)
	t.lineNum = 1

	return
}

func (t Tokenizer) size() int {
	return len(t.data)
}

func (t Tokenizer) left() int {
	return t.size() - t.offset
}

func (t Tokenizer) ok() bool {
	return t.left() > 0
}

func (t Tokenizer) current() byte {
	return t.data[t.offset]
}

func (t Tokenizer) peek() (next byte, ok bool) {
	if ok = t.ok(); ok {
		next = t.current()
	}

	return
}

func (t *Tokenizer) skip() {
	t.offset++
}

func (t *Tokenizer) read() (b byte, ok bool) {
	if b, ok = t.peek(); !ok {
		return
	}

	t.skip()

	return
}

func (t Tokenizer) rest() []byte {
	if !t.ok() {
		return []byte{}
	}

	return t.data[t.offset:]
}

func (t Tokenizer) index(b byte) int {
	return slices.Index(t.rest(), b)
}

func (t *Tokenizer) restTo(b byte) (data []byte, found bool) {
	index := t.index(b)
	if found = index > -1; !found {
		return
	}

	data = t.data[t.offset : t.offset+index]
	t.offset += index + 1

	return
}

func (t *Tokenizer) skipLine() {
	if index := t.index('\n'); index == -1 {
		t.offset = t.size()
	} else {
		t.offset += index
	}
}

func (t *Tokenizer) run() iter.Seq[Token] {
	return func(yield func(Token) bool) {
		var token Token

		iterations := 0

	loop:
		for {
			b, ok := t.read()

			iterations++
			if !ok {
				yield(newEOFToken())

				return
			}

			if iterations > t.size() {
				panic(fmt.Sprintf("content %q entered infinite loop", string(t.data)))
			}

			switch b {
			case '\n':
				t.lineNum++

				fallthrough
			case '\t', ' ':
				continue loop
			}

			f, found := mainLexemeIndex.find(lexemePrefix(b))

			if !found {
				token = newUnexpectedCharToken(t.lineNum, b)
			} else {
				token = f(t)
			}

			if token.tokenType == COMMENT {
				t.skipLine()

				continue loop
			}

			if !yield(token) {
				return
			}
		}
	}
}

func CmdTokenize(filename string) (code int, err error) {
	tokenizer, err := newTokenizer(filename)
	if err != nil {
		return
	}

	for t := range tokenizer.run() {
		if t.IsError() {
			code = 65

			fmt.Fprintln(os.Stderr, t)
		} else {
			fmt.Println(t)
		}
	}

	return
}
